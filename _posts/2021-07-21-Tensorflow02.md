---
layout: post
author: "Yan"
catalog: true
subtitle: "regression analysis"
header-img: "img/header/none3.jpg"
title: "텐서플로우로 회귀분석하기"
date: 2021-07-21 20:15:08 +0000
categories:
  - Machine_Learning
  - TensorFlow
  - tensor
  - regresssion_analysis
  - 회귀분석
comments: true
---

## 회귀분석 `Regression Analysis`

- 관찰된 연속형 변수들에 대해 두 변수 사이의 모형을 구한 뒤 적합도를 특정해 내는 분석 방법

### 기본 동작 원리

- 데이터들의 특성을 파악한다
- 경향성`Tendency` 및 의존성`Dependency`를 수식으로 작성한다
- 앞으로 발생할 일을 예측한다`Prediction`

#### 회귀분석 데이터의 특징

- 분석을 통해 나온 예측값과 실제 데이터 오차는 모든 데이터값(독립변수)에 대하여 동일한 분산을 가지고 있다.
- 데이터의 확률 분포는 정규분포를 이룬다.
- 독립변수 상호간에는 상관관계가 없다 == 선형적으로 독립한다
- 독립변수와 종속변수 사이에는 상관관계가 존재한다 == 선형관계다

#### 주요 용어

- 상관관계 : 두 변수 a, b가 있을 때 a값이 증가하거나 감소할 때 b의 값도 a갑의 영향으로 증가하거나 감소하게 된다.
- 독립변수 : 다른 변수에 영향을 받지 않는 변수다
- 종속변수 : 독립변수에 영향을 받아서 변화하는 변수다
- ex) 독립변수 : 시험 공부를 한 시간 / 종속변수 : 시험의 결과

### 선형 `Linearity` 데이터 모델

- 독립변수 x와 종속변수 y간의 관계가 1차식`y = ax + b`로 표현되는 것이 아니다
- 직선(1차식)이 아닐지라도 직선의 특징을 가지는 데이터 모델이다
- 중첩의 원리가 적용된다 -> 입력값과 출력값이 비례성 및 가산성을 가지게 되어 어느정도 예측이 가능하다
  - `y = ax + b`는 독립변수의 회귀계수인 기울기 `a`에 관하여 비례성을 가진 수식이다
- 파라미터(회귀계수)가 선형식으로 표현되는 회귀 모델이다

### 비선형 `Non Linearity` 데이터 모델

- 데이터(독립변수, 종속변수)를 변형하더라도 중첩의 원리를 적용한 수식으로 데이터를 표현한다
- `y = ax / (b + x)`
- 비선형 모델은 복잡한 패턴을 가지고 있기 때문에 예측이 불가능하며 이런 문제는 Deep Learing을 이용한다.

### 분류

#### 데이터 특성에 따른 분류

1. 데이터 특성에 따른 분류
- 선형 회귀분석 모델 `Linear Regression Analysis Model`
- 비선형 회귀분석 모델 `NonLinear Regression Analysis Model`

2. 독립변수 개수에 따른 분류
- 단순 회귀 분석 모델 : 독립변수 1개
- 다중 회귀 분석 모델 : 독립변수 2개 이상

3. 종속변수 개수에 따른 분류
- 단변량 회귀분석 모델 : 종속변수 1개
- 다변량 회귀분석 모델 : 종속변수 2개 이상

## 단순선형회귀분석 모델 `Single Variable Linear Regression`

- 목표 : 종속변수 Y(결괏값)와 독립변수 X(입력값)의 선형적 특성을 가지는 상관관계 모델을 생성하여 새로운 독립변수 X에 대한 결과를 예측

#### Linear Regression Model
- 학습 데이터 : 그래프에 표시된 점
- 학습 데이터의 특성을 대표하는 모델 : 그래프의 직선
- 독립변수 `X`(입력값) : 학습데이터의 `x` 축 값 
- 실제값 : 학습데이터의 `y` 축 값 
- 종속변수 `Y`(결괏값 or 예측값 or 가설값) : 학습 모델(직선) 수식에 대입한 독립변수 X의 값 

##### 수식 Y = W * + b
- 가설 수식이라고도 하며, 독립변수 X의 입력값을 넣어 계산하고, 결과값은 예측값이라고 부른다.
- W(Weight) 가중치
- b(bias) 편향

##### 실습

- tensorflow : 모델을 생성 하기 위한 라이브러리
- numpy : 행렬이나 일반적인 대규모 다차원 배열을 처리하는 라이브러리
- matplotlib : 그래프 생성 라이브러리

```python
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
from matplotlib import pyplot as plt
```

- 학습 데이터를 랜덤하게 생성하여 학습을 하기 위한 훈련용 데이터 수를 지정
- 학습률은 학습 모델 수식의 W(Weight)와 b(bias)의 최적의 값을 찾기 위한 최적화 함수의 입력 파라미터임
- 최적의 모델을 만들기 위한 총 학습 횟수 선언함

```python
##################
# 환경설정
##################
# 훈련용 데이터 수 선언
trainDataNumber = 100
# 모델 최적화를 위한 학습률 선언
leaningRate = 0.01
# 총 학습 횟수 선언
totalStep = 1001

##################
# 빌드단계
# 1. 학습 데이터 준비
##################
# 항상 같은 난수를 생성하기 위하여 시드 설정
np.random.seed(321)

# 학습 데이터 리스트 선언
xTrainData = list()
yTrainData = list()

# 학습 데이터 생성
xTrainData = np.random.normal(0.0, 1.0, size=trainDataNumber)

for x in xTrainData:
    # y 데이터 생성
    y = 10 * x + 3 + np.random.normal(0.0, 3)
    yTrainData.append(y)

# 학습 데이터 확인
plt.plot(xTrainData, yTrainData, 'bo')
plt.title("Train data")
plt.show()
```
- 모델 학습을 위한 학습데이터 생성
- numpy.random.normal() 을 이용하여 평균 0, 표준편차 1을 가지는 학습데이터 x(독립변수 x) 100개 생성
- 학습데이터의 실제 값을 구하기 위하여 y=10x+3+np.random.normal(0.0,3) 식에 학습데이터 x 를 대입하여 계산

```python
############################
# 빌드 단계
# 2. 모델 생성을 위한 변수 초기화
############################
# Weight 변수 선언
W = tf.Variable(tf.random_uniform([1]))
# Bias 변수 선언
b = tf.Variable(tf.random_uniform([1]))

# 학습데이터 xTrainData가 들어갈 플레이스 홀더 선언
X = tf.placeholder(tf.float32)
# 학습데이터 yTrainData가 들어갈 플레이스 홀더 선언
Y = tf.placeholder(tf.float32)
```
- Weight, bias 변수를 저장할 Variable 선언
- 학습데이터가 들어갈 placeholder 선언
- X : 학습데이터 x(독립변수)가 들어감
- Y : 학습데이터 x의 실제값(정답데이터)이 들어감

```python
##################################
# 빌드단계
# 3. 학습 모델 그래프 구성
##################################
# 3-1. 학습 데이터를 대표하는 가설 그래프 선언
# 방법1 : 일반 연산기호를 이용하여 가설 수식 작성
hypothesis = W * X + b
# 방법2 : tensorflow 함수를 이용하여 가설 수식 작성
# hypothesis = tf.add(tf.multiply(W, X), b)

# 3-2. 비용함수(오차함수, 손실함수) 선언
costFunction = tf.reduce_mean(tf.square(hypothesis - Y))

# 3-3. 비용함수의 값이 최소가 되도록 하는 최적화함수 선언
optimizer = tf.train.GradientDescentOptimizer(learing_rate=leaningRate)
train = optimizer.minimize(costFunction)
```
- 학습 데이터의 특성을 대표하는 가설 수식 작성
- 가설 수식에 학습데이터 x 의 값을 입력한 결괏값(예측값)과 실제값의 오차를 계산하는 비용함수(오차함수, - 손실 함수) 선언
- 비용함수의 값이 최소가 될 수 있도록 W, b의 최적값을 찾는 최적화 함수 선언  

- 가설 그래프 선언 목적 : 학습데이터를 대표하는 가설 그래프 선언
  - 1차 다항식의 직선 형태로 표현  

- 비용함수(오차함수, 손실함수) 선언 
  - 목적 : 가설 수식에 모든 학습데이터 X값을 입력하여 나온 종속변수 y값(가설값, 예측값, 결과값) 과 학습데이터의 실제값 Y의 오차를 계산하는 수식 작성  

- 최적화 함수 선언 
  - 목적 : 비용함수의 수식이 최소가 되는 W(Weight), b(Bias) 의 값을 찾는 최적화 함수 선언
  - 최적화 알고리즘 Gradient descent 사용
  - 최적화함수
    - 미분을 이용하여 스스로 최저 비용(오차)을 찾아가게 됨
    - 최적화 함수를 통하여 W, b의 변수를 변화시키게 됨
    - 오차가 최소가 되는 W, b 의 값을 찾아내는 과정을 통하여 최소 비용(오차)를 가지는 모델을 만듬

```python
######################################
# 실행단계
# 학습 모델 그래프를 실행
######################################
# 실행을 위한 세션 선언
sess = tf.Session()
# 최적화 과정을 통하여 구해질 변수 W, b 초기화
sess.run(tf.global_variables_initializer())

# 비용함수 그래프를 그리기 위한 변수 선언
WeightValueList = list()
costFunctionValueList = list()
```
- Session 변수(sess)를 선언
- 최적화 과정에서 계산되는 변수(W, b)의 초기화 
- 모델 학습 결과 확인을 위한 리스트 선언 

```python
print("------------------------------------------")
print("Train(Optimization) Start ")
# totalStep 횟수 만큼 학습
for step in range(totalStep):
    cost_val, W_val, b_val, _ = sess.run([costFunction, W, b, train],
                                         feed_dict={X: xTrainData, Y: yTrainData})
    # 학습 결과값을 저장
    WeightValueList.append(W_val)
    costFunctionValueList.append(cost_val)
    #학습 50회마다 중간 결과 출력
    if step % 50 == 0:
        print("Step : {}, cost : {}, W: {}, b : {}".format(step,
                                                           cost_val,
                                                           W_val,
                                                           b_val))
        # 학습 100회 마다 중간 결과 Fitting Line 추가
        if step % 100 == 0:
            plt.plot(xTrainData,
                     W_val * xTrainData + b_val,
                     label='Step : {}'.format(step),
                     linewidth=0.5)
    print("Train Finished")
```
- 실행단계 - 학습 모델 그래프 실행
  - totalStep 횟수 만큼 모델이 학습됨
  - sess을 통하여 최적화 함수를 계산하여학습 결과를 저장하고 출력함
- 학습 조건
  - 학습 데이터수 : 100개
  - 최적화 함수 : Gradient descent 알고리즘
  - 학습률 : 0.01
  - 학습 횟수 1,001회

```python
print("-----------------------------------------------")
print("[Train Result]")
# 최적화가 끝난 학습 모델의 비용함수 값
cost_train = sess.run(costFunction, feed_dict={X: xTrainData,
                                               Y: yTrainData})
# 최적화가 끝난 W, b 변수의 값
w_train = sess.run(W)
b_train = sess.run(b)
print("Train cost : {}, W : {}".format(cost_train, w_train, b_train))
print("-----------------------------------------------")
print("[Test Result]")

# 테스트를 위하여 x값 선언
textValue = [2.5]
# 최적화된 모델에 x에 대한 y값 계산
resultYValue = sess.run(hypothesis, feed_dict={X: testXValue})
# 테스트 결과값 출력
print("x value is {}, y value is {}".format(testXValue, resultYValue))
print("-----------------------------------------------")
```
- 모델 학습이 완료 후 비용함수의 값을 계산하고 W, b 변수의 값을 출력
- 학습 결과 확인을 위하여 X 값에 2.5를 입력하여 예측값을 출력

```python
# matplotlib를 이용하여 결과를 시각화
# 결과 확인 그래프
plt.plot(xTrainData,
         sess.run(W) * xTrainData + sess.run(b),
         'r',
         label='Fitting Line',
         linewidth=2)
plt.plot(xTrainData,
         yTrainData,
         'bo',
         label='Train data')
plt.legent()
plt.title("Train Result")
plt.show()

# 비용함수 최적화 그래프
plt.plot(WeightValueList, costFunctionValueList)
plt.title("costFunction curve")
plt.xlabel("Weight")
plt.ylabel("costFunction value")
plt.show()

# 세션 종료
sess.close()
```
![](/img/20210721-tensorflow02/1.jpg)  

## 다중선형회귀분석 모델 `Multi Variable Linear Regression `

- 목표 : 독립변수 2개, 종속 변수 1개를 가지는 Linear Regression 모델 학습

```python
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
from matplotlib import pyplot as plt
# 3차원 공간에서 그래프 출력
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

##################
# 환경설정
##################
# 훈련용 데이터 수 선언
trainDataNumber = 200
# 모델 최적화를 위한 학습률 선언
leaningRate = 0.01
# 총 학습 횟수 선언
totalStep = 1001


##################
# 빌드단계
# 1. 학습 데이터 준비
##################
# 항상 같은 난수를 생성하기 위하여 시드 설정
np.random.seed(321)

# 학습 데이터 리스트 선언
x1TrainData = list()
x2TrainData = list()
yTrainData = list()

# 학습 데이터 생성
x1TrainData = np.random.normal(0.0, 1.0, size=trainDataNumber)
x2TrainData = np.random.normal(0.0, 1.0, size=trainDataNumber)

for i in range(0, trainDataNumber):
    # y 데이터 생성
    x1 = x1TrainData[i]
    x2 = x2TrainData[i]
    y = 10 * x1 + 5.5 * x2 + 3 + np.random.normal(0.0, 3)
    yTrainData.append(y)

# 학습 데이터 확인
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot(x1TrainData,
        x2TrainData,
        yTrainData,
        linestyle="none",
        marker="o",
        mfc="none",
        markeredgecolor="red")
plt.show()

############################
# 빌드 단계
# 2. 모델 생성을 위한 변수 초기화
############################
# Weight 변수 선언
W1 = tf.Variable(tf.random_uniform([1]))
W2 = tf.Variable(tf.random_uniform([1]))
# Bias 변수 선언
b = tf.Variable(tf.random_uniform([1]))

# 학습데이터 xTrainData가 들어갈 플레이스 홀더 선언
X1 = tf.placeholder(tf.float32)
X2 = tf.placeholder(tf.float32)
# 학습데이터 yTrainData가 들어갈 플레이스 홀더 선언
Y = tf.placeholder(tf.float32)

##################################
# 빌드단계
# 3. 학습 모델 그래프 구성
##################################
# 3-1. 학습 데이터를 대표하는 가설 그래프 선언
hypothesis = W1 * X1 + W2 * X2 + b

# 3-2. 비용함수(오차함수, 손실함수) 선언
costFunction = tf.reduce_mean(tf.square(hypothesis - Y))

# 3-3. 비용함수의 값이 최소가 되도록 하는 최적화함수 선언
optimizer = tf.train.GradientDescentOptimizer(learing_rate=leaningRate)
train = optimizer.minimize(costFunction)

######################################
# 실행단계
# 학습 모델 그래프를 실행
######################################
# 실행을 위한 세션 선언
sess = tf.Session()
# 최적화 과정을 통하여 구해질 변수 W, b 초기화
sess.run(tf.global_variables_initializer())

# 학습 데이터와 학습 결과를 matplotlib를 이용하여 결과 시각화
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot(x1TrainData,
        x2TrainData,
        yTrainData,
        linestyle="none",
        marker="o",
        mfc="none",
        markeredgecolor="red")
Xs = np.arange(min(x1TrainData), max(x1TrainData), 0.05)
Ys = np.arange(min(x2TrainData), max(x2TrainData), 0.05)
Xs, Ys = np.meshgrid(Xs, Ys)

print("------------------------------------------")
print("Train(Optimization) Start ")
# totalStep 횟수 만큼 학습
for step in range(totalStep):
    cost_val, W1_val, W2_val, b_val, _ = sess.run([costFunction, W1, W2, b, train],
                                         feed_dict={X1: x1TrainData,
                                                    X2: x2TrainData,
                                                    Y: yTrainData})
    #학습 50회마다 중간 결과 출력
    if step % 50 == 0:
        print("Step : {}, cost : {}, W1: {},  W2: {}, b : {}".format(step,
                                                           cost_val,
                                                           W1_val,
                                                           W2_val,
                                                           b_val))
        # 학습 100회 마다 중간 결과 Fitting Line 추가
        if step % 100 == 0:
            plt.plot(Xs,
                     Ys,
                     W1_val * Xs + W2_val * Ys + b_val,
                     rstride=4,
                     cstride=4,
                     alpha=0.2,
                     cmap=cm.jet)
    print("Train Finished")
print("-----------------------------------------------")
# 결과 확인 그래프
plt.show()

# 세션 종료
sess.close()
```