---
layout: post
author: "Yan"
catalog: true
subtitle: "시스템의 안정성과 사용자 경험을 향상시키기"
header-img: "img/header/none2.jpg"
title: "Latency 관리 전략"
date: 2024-09-01 23:15:08 +0000
categories:
  - CS
tags:
  - latency
comments: true
---

## latency 관리 전략

Consumer가 트래픽을 감당하지 못해 지연(latency)이 발생할 때, 이를 완화하거나 해결하기 위한 다양한 전략이 있다. 시스템의 성능을 유지하고, 트래픽 급증 시에도 안정성을 보장하기 위해 설계하기 위함이다.  

일반적으로 **대용량 트래픽 처리와 관련된 latency 전략**에는 보통 이런 것들이 있다.

1. **로드 밸런싱 (Load Balancing)**
2. **캐싱 (Caching)**
3. **비동기 처리 및 메시지 큐 (Asynchronous Processing and Message Queues)**
4. **오토 스케일링 (Auto Scaling)**
5. **데이터베이스 최적화**
6. **콘텐츠 전송 네트워크 (CDN) 활용**
7. **백프레셔 (Backpressure)**
8. **서킷 브레이커 (Circuit Breaker)**
9. **폴백 메커니즘 (Fallback Mechanism)**
10. **엘라스틱 서치 및 로그 분석 도구 활용**


아래는 그 중 몇 가지에 대한 설명이다.

### 1. **Backpressure**

Backpressure는 생산자(Producer)와 소비자(Consumer) 간의 흐름 제어 메커니즘으로, 소비자가 처리할 수 있는 양만큼의 데이터를 요청하게 한다. 이를 통해 소비자가 트래픽을 감당하지 못할 때, 데이터의 생산 속도를 조절할 수 있다.

- **구현 방법**: Reactive Streams, Akka Streams, RxJava 등과 같은 리액티브 프로그래밍 라이브러리나 프레임워크에서 Backpressure를 지원한다.
- **장점**: 데이터 손실을 방지하면서도 소비자의 처리 능력에 맞춰 시스템을 조절할 수 있다.
- **단점**: 생산자와 소비자 간의 긴밀한 연동이 필요하며, 구현이 복잡할 수 있다.

### 2. **Rate Limiting**

Rate Limiting은 특정 시간 동안 소비자가 처리할 수 있는 요청 수를 제한하는 방법이다. 이를 통해 트래픽의 급증을 제어하고, 소비자가 감당할 수 있는 수준으로 요청을 분배할 수 있다.

- **구현 방법**: API Gateway, Load Balancer, 또는 메시지 큐에서 Rate Limiting을 설정한다.
- **장점**: 시스템의 과부하를 방지하고, 예측 가능한 성능을 유지할 수 있다.
- **단점**: 초과된 요청은 거부되거나 지연되므로, 사용자 경험에 영향을 미칠 수 있다.

### 3. **Queueing**

Queueing은 메시지나 요청을 큐에 저장하여 소비자가 처리할 수 있을 때까지 대기시키는 방법이다. 메시지 큐, 작업 큐, 또는 버퍼링 시스템을 사용해 트래픽을 완화할 수 있다.

- **구현 방법**: RabbitMQ, Kafka, SQS 등의 메시지 큐 시스템을 활용한다.
- **장점**: 트래픽 급증 시에도 데이터를 잃지 않고 처리할 수 있으며, 소비자가 처리 속도에 맞춰 작업을 수행할 수 있다.
- **단점**: 큐의 크기가 커질수록 지연(latency)이 증가할 수 있다.

### 4. **Auto-Scaling**

Auto-Scaling은 시스템의 부하에 따라 자동으로 인스턴스 수를 조정하는 전략이다. 클라우드 환경에서 특히 유용하며, 트래픽이 증가할 때 자동으로 더 많은 소비자 인스턴스를 생성해 부하를 분산시킨다.

- **구현 방법**: AWS EC2 Auto Scaling, Kubernetes Horizontal Pod Autoscaler 등을 사용한다.
- **장점**: 트래픽 급증에 유연하게 대응할 수 있으며, 필요 시 자원을 자동으로 할당해 성능을 유지할 수 있다.
- **단점**: Auto-Scaling이 활성화되기까지 시간이 소요될 수 있으며, 과도한 리소스 사용으로 비용이 증가할 수 있다.

### 5. **Circuit Breaker**

Circuit Breaker는 시스템이 과부하 상태에 있을 때, 실패한 요청에 대한 재시도를 일시적으로 중단하고 대체 동작을 수행하는 전략이다. 이를 통해 시스템이 완전히 중단되는 것을 방지할 수 있다.

- **구현 방법**: Netflix Hystrix, Resilience4j 등의 라이브러리를 사용한다.
- **장점**: 시스템의 안정성을 높이고, 실패한 요청이 시스템 전체에 영향을 미치지 않도록 한다.
- **단점**: 요청이 실패할 경우 대체 경로를 제공해야 하므로, 설계 복잡성이 증가할 수 있다.

### 6. **Load Shedding**

Load Shedding은 시스템이 처리할 수 없는 트래픽을 능동적으로 거부하는 전략이다. 시스템이 과부하 상태일 때, 덜 중요한 요청을 버리거나 지연시켜 중요한 요청이 처리되도록 한다.

- **구현 방법**: 서비스 수준 협정(SLA)에 따라 요청의 우선순위를 지정하고, 중요하지 않은 요청을 제한한다.
- **장점**: 중요한 트래픽을 우선적으로 처리할 수 있어 시스템 성능을 보장한다.
- **단점**: 버려진 요청으로 인해 사용자 경험이 저하될 수 있다.

### 7. **Graceful Degradation**

Graceful Degradation은 시스템이 과부하 상태에서 성능을 저하시키더라도 최소한의 기능을 유지하는 전략이다. 중요한 기능은 유지하면서 덜 중요한 기능을 제한하는 방식으로 작동한다.

- **구현 방법**: 핵심 기능을 우선 처리하고, 비핵심 기능의 성능을 조정하거나 일시적으로 비활성화한다.
- **장점**: 시스템이 완전히 중단되는 것을 방지하고, 사용자에게 최소한의 서비스를 제공할 수 있다.
- **단점**: 일부 기능이 비활성화되거나 성능이 저하될 수 있다.


> Consumer가 트래픽을 감당하지 못할 때, 다양한 latency 관리 전략을 통해 시스템의 안정성을 유지할 수 있다. 각 전략은 특정 상황에 최적화되어 있으며, 시스템의 요구 사항과 트래픽 특성에 따라 적절한 전략을 선택하고 결합해 사용할 수 있다. Backpressure나 Queueing 같은 방법은 트래픽을 효과적으로 관리하고, Auto-Scaling이나 Circuit Breaker는 시스템의 안정성을 강화하는 데 유용하다.

###### reference


> [서비스 가용성 확보에 필요한 Rate Limiting Algorithm에 대해](https://www.mimul.com/blog/about-rate-limit-algorithm/)